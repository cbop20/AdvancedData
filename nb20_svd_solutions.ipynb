{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 20: Singular Value Decomposition\n",
    "***\n",
    "\n",
    "In this notebook, we will decompose a delicious and scientific matrix using singular value decomposition, and perform dimension reduction by discarding the components that have low importance, as measured by their singular values.\n",
    "\n",
    "We'll need numpy for this notebook, so let's load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 1: Computing SVD\n",
    "\n",
    "Suppose you have developed technology to reanimate dead scientists and survey their modern-day cereal preferences, where a rating of 0 indicates they have not tried the cereal, 1 is a bad rating and 5 is a good one. You obtain the following data matrix:\n",
    "\n",
    "$$\\begin{array}{c|ccc}\n",
    "                   & \\text{Wheaties} & \\text{Lucky Charms} & \\text{Cookie Crisp} & \\text{Weetabix} \\\\\n",
    "   \\hline\n",
    " \\text{Ampere}       & 5 & 1 & 0 & 3 \\\\\n",
    " \\text{Bernoulli}    & 2 & 2 & 5 & 1 \\\\\n",
    " \\text{Curie}        & 5 & 0 & 0 & 4 \\\\\n",
    " \\text{Darwin}       & 0 & 4 & 3 & 0 \\\\\n",
    " \\text{Eratosthenes} & 0 & 2 & 3 & 0 \\\\\n",
    " \\text{Fisher}       & 4 & 0 & 0 & 3 \\\\\n",
    " \\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[5,1,0,3],\n",
    "              [2,2,5,1],\n",
    "              [5,0,0,4],\n",
    "              [0,4,3,0],\n",
    "              [0,2,3,0],\n",
    "              [4,0,0,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll get to computing SVD by hand using the methods from lecture in a moment, but first it would be nice to have a method for checking whether our future calculation is correct or not. Numpy to the rescue! Specifically, `numpy.linalg.svd` will take a matrix $M$ we wish to decompose using SVD, and return the $U$, $\\Sigma$ and $V^T$ matrices (more or less), such that $M = U\\Sigma V^T$. Check out the documentation for [numpy.linalg.svd](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html) for more details on the return values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values are:  [10.4609  7.6557  2.1351  0.6334]\n"
     ]
    }
   ],
   "source": [
    "U, Sig, VT = np.linalg.svd(M)\n",
    "print(\"Singular values are: \", np.round(Sig, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shapes of the three arrays that are returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 6)\n",
      "(4,)\n",
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "print(U.shape)\n",
    "print(Sig.shape)\n",
    "print(VT.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in class, the $U$ matrix would have had shape $6 \\times 4$, but here we have $6\\times 6$. What's up with that?\n",
    "\n",
    "Recall that the columns of $U$ are the eigenvectors of $MM^T$. What size is that matrix, and how many eigenvectors should it have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "$M$ is $6 \\times 4$, so $MM^T$ is $6 \\times 6$. So, it should have 6 eigenpairs. Thus, $U$ is of the proper size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to recover the original matrix $M$ and make sure our SVD is working properly, we would like to multiply $U \\Sigma V^T$, but the sizes aren't right. First off, $\\Sigma$ should actually be a diagonal matrix with the singular values down the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.46087701  0.          0.          0.        ]\n",
      " [ 0.          7.65572277  0.          0.        ]\n",
      " [ 0.          0.          2.13513866  0.        ]\n",
      " [ 0.          0.          0.          0.63335925]]\n"
     ]
    }
   ],
   "source": [
    "Sig = np.diag(Sig)\n",
    "print(Sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying $\\Sigma$ on the right by $V^T$ is valid, since the number of columns of $\\Sigma$ is 4, which matches the number of rows in $V^T$. But multiplying on the left by $U$ won't work since the number of rows in $\\Sigma$ is only 4, whereas $U$ contains all 6 eigenvectors of $MM^T$. We can remedy this by padding $\\Sigma$ with extra rows of $0$s to match the number of columns in $U$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.46087701  0.          0.          0.        ]\n",
      " [ 0.          7.65572277  0.          0.        ]\n",
      " [ 0.          0.          2.13513866  0.        ]\n",
      " [ 0.          0.          0.          0.63335925]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# full Sigma matrix should match U's columns and V's rows\n",
    "Sig_full = np.zeros((U.shape[1], VT.shape[0]))\n",
    "\n",
    "# fill in the singular values\n",
    "Sig_full[:Sig.shape[0], :Sig.shape[1]] = Sig\n",
    "\n",
    "print(Sig_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the matrix product $U \\Sigma V^T$, using our padded $\\Sigma$ matrix and the output from the `np.linalg.svd` function call. How well does it match the original $M$ matrix? Eyeballing this is difficult, so you may want to use a popular matrix norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1639882322073053e-14\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# we compute the frobenius norm (default for np.linalg.norm)\n",
    "print(np.linalg.norm(M - np.matmul(np.matmul(U, Sig_full), VT)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 2: A Journey into Concept Space\n",
    "\n",
    "Recall that a singular value decomposition can be thought of as rotating our original data into ***concept space*** and performing dimension reduction by only retaining the most important concepts (i.e., the highest singular values). Based on the original data set, what do you think the concepts are? If you aren't sure, try simply performing a Google Image Search for the four cereals in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, rotate the original data matrix $M$ into concept space by multiplying on the right by $V$. Discuss with those around you the cereal preferences of the scientists. Which scientist(s) display strong preference for one type of cereal over another? Which scientist(s) are not as picky? Is it ironic that any of these scientists does not seem to display a binary love-it-or-hate-it relationship with types of cereal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.72658974 -1.23291163 -0.69463608  0.45119793]\n",
      " [-3.69733755  4.30012552  1.34825993  0.14425925]\n",
      " [-6.07450887 -1.98941548  0.10929952 -0.36141622]\n",
      " [-1.50244804  4.52833007 -1.49042764 -0.12450749]\n",
      " [-1.1121463   3.42507916  0.13145117 -0.12117721]\n",
      " [-4.75099309 -1.55055671  0.08884038 -0.12627712]]\n"
     ]
    }
   ],
   "source": [
    "V = np.transpose(VT)\n",
    "MV = np.matmul(M, V)\n",
    "print(MV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the columns of the matrix $MV$ corresponds to the scientists' preferences along each of the concepts. Examine the magnitudes (don't worry about positive or negative values) for each concept. Which concept(s) do you hypothesize are important to retain in our reduced-rank representation of the data set, and which concept(s) do you think we can safely discard? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "The first two concepts have strengths greater than 1, so we should probably keep those. The last concept has small strengths, all less than 1, so we can probably safely get rid of that one. The third concept is a bit of a toss-up, so we should compute the energy (sum of singular values squared) to figure out whether or not to keep it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 3: Reducing Dimensionality\n",
    "\n",
    "Compute the proportion of the total energy ($\\sum \\sigma_k^2$, where $\\sigma_k$ is the $k^{th}$ singular value) that is retained by eliminating the 1, 2 or 3 lowest singular values. As a general guideline, when we reduce dimensionality using SVD, we aim to keep about 80-90% of the total energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "total_energy = 0 # TODO -- sum the squares of all singular values\n",
    "frac_energy_retained = np.zeros(len(Sig)) # initialize\n",
    "for k in range(1,len(frac_energy_retained)+1):\n",
    "    frac_energy_retained[k-1] = 0 # TODO - ratio of (sum of k largest singular values) to total energy\n",
    "\n",
    "print(frac_energy_retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63254305 0.97132971 0.99768125 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "total_energy = np.sum(Sig**2)\n",
    "frac_energy_retained = np.zeros(len(Sig))\n",
    "for k in range(1,len(frac_energy_retained)+1):\n",
    "    frac_energy_retained[k-1] = np.sum(Sig[:k,:k]**2)/total_energy\n",
    "    \n",
    "print(frac_energy_retained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which components should we keep in our reduced rank representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "The components along the first two singular vectors account for 97% of the energy, so we should be safe to discard the last two components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the reduced rank approximation of $M$ by zero-ing out the columns of $U$, rows of $V^T$, and elements of $\\Sigma$ associated with the unimportant concepts. What is the distance between the reduced rank matrix and the original data matrix, using the Frobenius norm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = 0 # TODO! decide how many to keep based on the energy\n",
    "U_red = U[:,:keep]\n",
    "Sig_red = Sig_full[:keep,:keep]\n",
    "VT_red = VT[:keep,:]\n",
    "M_red = np.matmul(np.matmul(U_red, Sig_red), VT_red)\n",
    "print(M_red)\n",
    "\n",
    "print(np.linalg.norm(M_red-M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.7577699   0.43744354  0.42369952  3.36253603]\n",
      " [ 1.87942647  3.09359732  4.21768531  1.12691028]\n",
      " [ 5.20630897  0.08803348 -0.07807664  3.70647177]\n",
      " [ 0.11303884  2.79114616  3.86617955 -0.11182255]\n",
      " [ 0.06656856  2.10639716  2.91841412 -0.09775144]\n",
      " [ 4.07069234  0.07183389 -0.05688262  2.89779738]]\n",
      "2.2270969959294824\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "keep = 2\n",
    "U_red = U[:,:keep]\n",
    "Sig_red = Sig_full[:keep,:keep]\n",
    "VT_red = VT[:keep,:]\n",
    "M_red = np.matmul(np.matmul(U_red, Sig_red), VT_red)\n",
    "print(M_red)\n",
    "\n",
    "print(np.linalg.norm(M_red-M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Frobenius norm itself may not mean much to you. Sometimes it is more meaningful to express this reproduction error as a fraction of the norm of the data matrix itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16932304913072402\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(M_red-M)/np.linalg.norm(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
